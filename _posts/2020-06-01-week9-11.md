---
layout: post
title: Semester 1 | Weeks 9-10
---

In order to develop the model, I worked through a cluster, which consists of interconnected computers or servers that work together to offer high-performance computing resources. With the secure shell protocol, I can remotely access the genome center server over the internet. Once connected to the cluster, I can access shared files and, more importantly, harness the cluster's resources to train my model.

One of the benefits of the cluster is access to the graphical processing unit, or GPU to train models, ensuring efficient training regardless of my local computational resources. GPUs excel at training models on large datasets, as their architecture is well-suited for matrix operations. They also enable batch processing, which involves processing groups of samples simultaneously rather than sequentially. Training data is typically divided into smaller subsets or batches containing a fixed number of samples. GPUs can process multiple batches in parallel, and the Pytorch library we use for training our model is optimized for GPU usage.

I will be using python to do most of my data processing and model training. Python is a popular language for machine learning due to its simplicity and the availability of powerful machine learning libraries. These libraries provide pre-built functions and algorithms for creating, training, and deploying machine learning models. Some popular libraries include Scikit-learn, Pytorch, TensorFlow, and Keras. Using these libraries allows us to experiment with our data and model without starting from scratch, while also offering evaluation tools to assess model performance after training.

Without these libraries, developing and deploying a model would be significantly more time-consuming. For example, building a neural network would require manually defining layer structures, layer types, neuron numbers, activation functions, and loss functions. This process can lead to a deeper understanding of machine learning, but to the novice, can be extremely error-prone. For our project, we will use the Pytorch machine learning library. 

In previous journal entries, we discussed partitioning a dataset into training and test sets. Alternatively, we can use DataLoaders, utilities that simplify loading data from a dataset and feeding it to a model during training. DataLoaders provide methods to iterate over the data in batches, shuffle the data, and load the data in parallel. They also promote model reproducibility, allowing other researchers to replicate results because the same data is loaded and processed each time the model is trained. DataLoaders also offer options for shuffling data or applying augmentation methods.

To utilize the DataLoader, we will write a function to one-hot encode genome values from the indices specified in our training data. One-hot encoding is a technique that represents categorical variables as numerical values, which our model can process. Categorical data, such as a string of nucleotide bases like "ACCCBA," must be represented mathematically. One-hot encoding converts the string of base pairs into a matrix of ones and zeros, which we will use for convolution.

The junction start and end coordinates correspond to indices in the genome where the junction begins. We will map this to our genome reference file to obtain the nucleotide encodings for the junction site. Since the junction site itself represents a single index, passing this into a one-hot encoding function will produce a vector for a single nucleotide. Instead, we will extract 20 sequences to the left and 50 to the right of the junction, resulting in a total sequence length of 80 for the starting junction. We will do the same for the ending junction and concatenate the resulting base pair string sequence. After inputting this into our one-hot encoder, we obtain our matrix. We will iterate over all rows of our dataset, with the DataLoader returning our x and y values, which represent the matrices and junction usage ratio, respectively.

Sources: 

https://pytorch.org/tutorials/beginner/basics/data_tutorial.html


