---
layout: post
title: Semester 1 | Weeks 6-8
---

In addition to having an understanding of genomics, I also had to learn to create, train, and test machine learning models. During my research, I was also concurrently taking machine learning, deep learning, and artificial intelligence. The classes helped me understand artificial intelligence and machine learning as a whole, and I used my skills to process my data, create my model, and eventually train the model on my dataset. AI and machine learning can seem like an intimidating concept, so I have defined some of the basic terms to help with understanding. 

**Artificial Intelligence** - A non-human program or model that can solve sophisticated tasks

**Machine Learning** - A subfield of artificial intelligence.  A program or system that trains a model from input data. The trained model can make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model.

**Model** - Any mathematical construct that processes input data and returns output. A set of parameters and structure needed for a system to make predictions.

**Training** - Feeding the model with a dataset (in our case our dataset is labeled). The model recognizes patterns and relationships in the data by adjusting its parameters based on the input data and the corresponding output. 

**Data Preprocessing** - Cleaning and transforming the data to prepare it for training. 

**Array** - A data structure that is a collection of data that is organized in a grid-like structure. Arrays can have multiple dimensions and can store a variety of different types of data (integers, strings, objects).  Arrays can be N-dimensional. A one dimensional array is called a list or vector, while a two dimensional array is called a table or Matrix. Matrices higher than two dimensions are called tensors. Matrices are powerful because they allow advanced operations such as matrix multiplication and inversion, which are important properties that make machine learning powerful. 

**Neural Networks** - A type of model inspired by the human brain, they consist of interconnected layers of artificial neurons (nodes). Each node processes input data and passes it through the network to produce an output. All nodes connect to all of the nodes in the next layer, with a weight scalar value. 

**Convolution/Convolutional Neural Networks** - Convolution is a mathematical operation that combines input data, usually in the form of a matrix, and performs mathematical operations with a convolutional filter to train weights of a network. Layers of the network manipulate the input layer by layer, ending with an output layer that can be used for tasks ranging from classification or feature mapping. 

**Supervised Learning** - When a model is trained on a labeled dataset, usually containing an input output pair. Models learn to map inputs to outputs by minimizing the difference between predictions and true outputs. 

These definitions provide an overview of artificial intelligence and machine learning. We will go more in depth with model development and the tools we use in later weeks. In order to build our model, we need our training data (in our case a file with data in a tabular format) as well as a fully sequenced genome for reference to build our matrices. The first step is to take the training data file and put it into a format that the environment we are working in can understand. To do this, we will use the pandas library, which is a data manipulation library in Python. Specifically, we will use the pandas DataFrame object that represents our training file to organize our tabular data into rows and columns with labeled headers. 

Once our training data is formatted for manipulation, we can initiate data exploration, which helps us better understand the data used for training our models. Models learn from the data researchers provide and make predictions using this trained dataset. Thus, it's crucial that the dataset accurately represents the problem we aim to solve.

For example, take a dataset in a table form representing patients with heart disease, where each column signifies a health feature, such as blood pressure, age, and weight. The final column is a binary value, "yes" or "no," indicating if the patient has heart disease. It's the researcher's responsibility to evaluate the dataset's quality. If the distribution is 99% positive for heart disease and 1% negative, the researcher should recognize this as an imbalanced dataset, impacting the model's performance and reliability. In this example, the model becomes biased towards the majority class, leading to poor performance in detecting the minority class and hindering its generalization capabilities for unseen data.

Datasets can be unbalanced, and researchers can address this by employing methods like oversampling the minority class, undersampling the majority class, or generating new data points through synthetic minority over-sampling techniques. In our dataset, it is essential to examine the data points to see if anything is missing or needs to be modified. 

Our training set is a CSV file with six columns: index, chromosome name, starting junction location, ending junction location, cluster label, and clusterCount. The last column contains a list of integers. The training data features 30,746 unique rows in total. Our objective is to develop a model capable of predicting a junction use ratio, which is not currently present in the table. To achieve this, we must extract the sum of counts at a specific junction divided by the sum of counts for the cluster. I created new columns for the sum of values inside each array and a new "junction" column, which combines the starting and ending junction indices.

To calculate the junction use ratio, I loop through all rows in the dataframe, summing the counts for each junction. This serves as the numerator for our junction usage ratio calculation. The denominator is the sum of counts for each cluster. After calculating the ratio, I filled a new dataframe object with the junction start, end, chromosome, and usage ratio. This new dataframe now contains the information needed to create our training and test data.

I conducted data exploration and preprocessing in a single Jupyter notebook, creating functions to clean chromosome names, cluster names, and format the cluster count. I later streamlined my data preprocessing by creating dedicated Python files for my functions and using a data pipeline, which returns the matrix and junction usage ratio required for training.

Sources: 

https://developers.google.com/machine-learning/glossary



